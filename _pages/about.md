---
permalink: /
title: "Vibhas Kumar Vats"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

About me
------

I am a Ph.D. candidate at [Indiana University Bloomington](https://www.indiana.edu/). I work in [Computer Vision Lab](http://vision.soic.indiana.edu/) with [Prof. David Crandall](https://homes.luddy.indiana.edu/djcran/). I am majoring in [Computer Science](https://cs.indiana.edu/) with a minor in Machine Learning. Before joining the Ph.D. program, I earned a master's degree in [Data Science](https://datascience.indiana.edu/programs/residential/index.html) at Indiana University with a thesis on ['Response-Based Knowledge Distillation'](https://vkvats.github.io/files/Vkvats_master_thesis.pdf). I completed a Bachelor's degree in Electrical Engineering at the National Institute of Technology, Patna [(NITP)](http://www.nitp.ac.in/php/home.php). 

[(Résumé-long)](https://vkvats.github.io/files/VibhasVats-resume-public.pdf).  

Research Interests
------

* 3D reconstruction with learning-based Multi-View Stereo
* Geometrical reasoning of 3D scenes
* Deep Learning and Case-Based Reasoning (DL-CBR) integration
* Deep Learning - Architectural understanding and exploration 

Publications
------
**Peer-reviewed Papers**

* Z. Wilkerson, Vibhas Vats, David Leake, David Crandall, "Extracting Indexing Features for CBR from Deep Neural Networks: A Transfer Learning Approach", Accepted - ICCBR 2024
* Vibhas Vats, Sripad Joshi, David Crandall, Md. Reza, Soon-Heung Jung, "GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo", WACV 2024 [(Project Page)](https://vkvats.github.io/GCMVSNet-page/)
* Zachary Wilkerson, Vibhas Vats, Karan Acharya, David Leake, David Crandall, "Examining the Impact of Network Architecture on Extracted Feature Quality for CBR" ICCBR-2023 [(Paper)](http://vision.soic.indiana.edu/papers/examining2023iccbr.pdf)
* Vibhas Vats, David Crandall (2021). "Controlling the Quality of Distillation in Response-Based Network Compression." AAAI-22. 1-8. [(Paper)](https://arxiv.org/abs/2112.10047)
* VK Vats, S Rai, S De, M De (2018). "Mitigating Effect of Communication Link Failure in Smart Meter-Based Load Forecasting." Springer. 289-300. [(Paper)](https://vkvats.github.io/publication/mitigating-springer-singapore-2018)
* VK Vats, S Rai, D Bharti, Mala De. (2018). "Very Short-term, Short-Term and Mid-Term Load Forecasting for Residential Academic Institute: A Case Study." IEEE. 1-6. [(Paper)](https://vkvats.github.io/files/paper1.pdf)

**PrePrint**
* Vibhas Vats and David Crandall, "Geometric Constraints in Deep Learning Frameworks: A Survey" March-2024. [(Paper)](https://arxiv.org/abs/2403.12431)


**Under-review**
* Cuhua Wang, Md. Reza, Vibhas Vats, Y. Ju, N. Thankurdesai, Y. Wang, D. Crandall, J. Seo, Soon-Heung Jung, "Deep Learning-based 3D Reconstruction from Multiple Images: A Survey" Under-review Neurocomputing

**Patents**
------
* Soon-Heung Jung, David Crandall, Vibhas Kumar Vats, Shaurya Shubham, Md Alimoor Reza, Chuhua Wang, "Method and Apparatus for Estimating Depth Information of Images", USA [Google Patents link](https://patents.google.com/patent/US20230326051A1/en)

**Master's Thesis**
* Response-Based Knowledge Distillation. [(pdf)](https://vkvats.github.io/files/Vkvats_master_thesis.pdf)

Please see my [Google Scholar Profile](https://scholar.google.com/citations?user=aRoPd9gAAAAJ&hl=en&authuser=5)


Teaching
------

I co-teach Computer Vision (CSCI-B 657) with my advisor every Spring since 2022. I discuss a number of seminal papers exploring deep learning (DL) architectures. I designed the first iteration of the DL discussion section in Spring 2022, it is designed to explore the seminal papers exploring DL architectures to develop an intuitive as well as mathematical understanding of major concepts in DL. In 16 weeks, the course roughly covers major architectures in three broad sections, Convolution-based networks (CNNs), Multi-layer perceptron-based networks (MLPs), and Transformer-based networks (ViTs). In CNNs, we cover models like LeNet, AlexNet, GoogLeNet, ResNets, Wide ResNet, Stochastic ResNet, ResNeXt, DenseNet, ConvMixer, and Xception Net. In MLPs, we cover models like MLP Mixer, ResMLP, cycleMLP, and S^2 MLP model, and in ViTs, we cover models like Vision Transformer, DeiT, Swin Transformer, Local ViT, Convolutional-ViT, etc. We also explore shift operation-based networks. The class meets weekly with more than 150 students participating in the discussion. The list of papers covered in the discussion is below  

[Spring 2022 discussion list](https://vkvats.github.io/files/B657-discussion-papers-Spring22.pdf) | [Slides](https://drive.google.com/drive/folders/1vEXb8_-DZn4HFVfo7bC_Q6CvqAJlRPCB?usp=sharing) | Videos
[Spring 2023 discussion list](https://vkvats.github.io/files/B657-Discussion-Spring23.pdf) | [Slides](https://drive.google.com/drive/folders/1NM3sf78nHQWDuvTeW9pzujgO4wAFmvrE?usp=sharing) | Videos
[Spring 2024 discussion list](https://vkvats.github.io/files/B657-Discussionpapers-Spring24.pdf) | [Slides](https://drive.google.com/drive/folders/1hPQx0gzOSjBbo-RJ1WNr5TegSNI1mOJI?usp=sharing) | [Videos]()

**Preprints**

Preprints are available on [arXiv](https://arxiv.org/a/vats_v_1.html)


Service
------

**Conference Program Committee / Reviewer**

* International Conference on Intelligence Science (ICIS) - 2022


Fun
------
I run for fun :). Find me on the Nike Run Club (@Vibhas_Vats).
